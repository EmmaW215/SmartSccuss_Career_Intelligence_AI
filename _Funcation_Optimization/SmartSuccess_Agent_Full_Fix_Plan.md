# SmartSuccess Interview Agents â€” Full Fix Plan

> **Date:** February 15, 2026  
> **Source:** `SmartSuccess_Mock_Interview_Agents_Analysis.md` + `SmartSuccess_Customize_Interview_Reassessment.md`  
> **Scope:** All 4 interview agents (Phase 1 + Phase 2) + cross-cutting infrastructure  
> **Guiding Principles:** Minimum cost Â· Minimum change effort Â· Minimum impact to existing features

---

## Table of Contents

1. [Fix Plan Overview & Priority Matrix](#1-fix-plan-overview--priority-matrix)
2. [Module A: Cross-Agent Shared Fixes](#2-module-a-cross-agent-shared-fixes)
3. [Module B: Screening Interview Agent Fixes](#3-module-b-screening-interview-agent-fixes)
4. [Module C: Behavioral Interview Agent Fixes](#4-module-c-behavioral-interview-agent-fixes)
5. [Module D: Technical Interview Agent Fixes](#5-module-d-technical-interview-agent-fixes)
6. [Module E: Customize Interview Agent Fixes (Phase 2)](#6-module-e-customize-interview-agent-fixes-phase-2)
7. [Module F: Architecture & Infrastructure Fixes](#7-module-f-architecture--infrastructure-fixes)
8. [Module G: Prompt Engineering Fixes](#8-module-g-prompt-engineering-fixes)
9. [Module H: User Experience Fixes](#9-module-h-user-experience-fixes)
10. [Module I: Data & Evaluation Quality Fixes](#10-module-i-data--evaluation-quality-fixes)
11. [Implementation Roadmap](#11-implementation-roadmap)
12. [Cost Impact Summary](#12-cost-impact-summary)

---

## 1. Fix Plan Overview & Priority Matrix

### Issue Classification Summary

| Source | Critical/High | Medium | Low | Total |
|--------|:---:|:---:|:---:|:---:|
| Cross-Agent (Q1â€“Q7) | 1 | 4 | 2 | 7 |
| Screening (S1â€“S3) | 0 | 2 | 1 | 3 |
| Behavioral (B1â€“B4) | 1 | 2 | 1 | 4 |
| Technical (T1â€“T5) | 1 | 3 | 1 | 5 |
| Customize â€” Phase 2 (R1â€“R4, A1â€“A2, E1â€“E2, I1â€“I2) | 1 | 6 | 3 | 10 |
| Architecture (A1â€“A4) | 1 | 2 | 1 | 4 |
| Prompt Engineering (P1â€“P4) | 0 | 3 | 1 | 4 |
| User Experience (U1â€“U4) | 0 | 1 | 3 | 4 |
| Data & Evaluation (D1â€“D3) | 0 | 1 | 2 | 3 |
| **Total** | **5** | **24** | **15** | **44** |

### Sprint Allocation

| Sprint | Focus | Duration | Issues Addressed |
|--------|-------|----------|------------------|
| Sprint 1 | Critical/High â€” safety & correctness | 1 week | Q4, B1, T1, Infra-A1, Phase2-A2 |
| Sprint 2 | Shared foundation â€” prompt & LLM layer | 1 week | Q1â€“Q3, Q5, P1â€“P3, Q2 (LLM centralization) |
| Sprint 3 | Agent-specific medium fixes | 1 week | S1, S3, B2, B3, T2, T3, T4 |
| Sprint 4 | Customize Interview enhancements | 1 week | R1, R2, E1, E2, I1, Phase2-A1 |
| Sprint 5 | UX, low-priority, polish | 1 week | U1â€“U4, Q6, Q7, S2, B4, T5, R3, R4, D1â€“D3, P4, I2, A2â€“A4 |

---

## 2. Module A: Cross-Agent Shared Fixes

These issues affect all 3 Phase 1 agents (Screening, Behavioral, Technical) and should be fixed ONCE in the shared `BaseInterviewService` or shared utilities. Fixing them at the base level automatically propagates to all agents with zero per-agent work.

---

### A-Q4: No Conversation Context in Evaluation ðŸ”´ High

**Problem:** Each response is evaluated in isolation. The LLM evaluator never sees previous Q&A pairs, so it cannot assess improvement, consistency, contradictions, or conversation arc.

**Root Cause:** `evaluate_response()` in each `*_interview.py` sends only the current question + response to the LLM.

**Fix â€” Inject Conversation History into Evaluation Prompt:**

```
File: smartsuccess-interview-backend/interview/base_interview.py
Method: evaluate_response() (or wherever each agent calls the LLM for evaluation)
```

**Implementation:**

```python
# In BaseInterviewService or a shared evaluation utility

def _build_evaluation_context(self, session: InterviewSession, current_index: int) -> str:
    """Build conversation history string for evaluation context."""
    history_lines = []
    # Include up to last 3 Q&A pairs for context (keeps token cost low)
    start = max(0, current_index - 3)
    for i in range(start, current_index):
        q = session.questions_asked[i] if i < len(session.questions_asked) else ""
        r = session.responses[i] if i < len(session.responses) else ""
        history_lines.append(f"Q{i+1}: {q}\nA{i+1}: {r}")
    return "\n\n".join(history_lines)
```

Then prepend to the evaluation prompt:

```python
context_block = self._build_evaluation_context(session, current_question_index)
evaluation_prompt = f"""
## Previous Q&A Context (for consistency and progression assessment):
{context_block if context_block else "This is the first question."}

## Current Question and Response to Evaluate:
Question: {current_question}
Response: {user_response}

{existing_evaluation_instructions}
"""
```

**Cost Impact:** ~200â€“400 extra tokens per evaluation call (last 3 Q&A pairs). At Gemini free tier pricing: $0.

**Change Scope:** 1 shared method + 1 line change per agent's evaluation call (3 agents).

**Risk:** Low â€” adds context prefix to existing prompt; does not change response schema.

---

### A-Q1: Prompt Duplication (Dead Code) ðŸŸ  Medium

**Problem:** Evaluation prompts exist BOTH inline in `*_interview.py` AND in `prompts/*_prompts.py`. The prompts files are never imported â€” they are dead code.

**Fix â€” Consolidate to Single Source of Truth:**

**Option A (Recommended â€” Minimum change):** Delete the `prompts/` directory entirely. The inline prompts in `*_interview.py` are the ones actually executing. Removing unused files has zero runtime impact.

```bash
# Delete dead code files
rm smartsuccess-interview-backend/prompts/screening_prompts.py
rm smartsuccess-interview-backend/prompts/behavioral_prompts.py
rm smartsuccess-interview-backend/prompts/technical_prompts.py
```

**Option B (Better long-term):** Extract inline prompts INTO the `prompts/` files, then import them. This centralizes prompt management but requires touching all 3 agent files.

```python
# prompts/screening_prompts.py
SCREENING_EVALUATION_PROMPT = """..."""
SCREENING_FOLLOWUP_PROMPT = """..."""
SCREENING_SUMMARY_PROMPT = """..."""

# interview/screening_interview.py
from prompts.screening_prompts import SCREENING_EVALUATION_PROMPT
# ... use imported constants instead of inline strings
```

**Recommendation:** Go with Option A now (Sprint 2), refactor to Option B if prompt iteration becomes frequent.

---

### A-Q2: No Centralized LLM Service Usage ðŸŸ  Medium

**Problem:** Each agent creates its own `AsyncOpenAI` client directly, bypassing `llm_service.py` which has the fallback chain (Gemini â†’ Groq â†’ OpenAI).

**Fix â€” Route All LLM Calls Through `llm_service.py`:**

```python
# In BaseInterviewService.__init__() or a shared module

from services.llm_service import LLMService

class BaseInterviewService:
    def __init__(self):
        self.llm = LLMService()  # Uses existing fallback chain
        # Remove: self.client = AsyncOpenAI(...)
    
    async def _call_llm(self, system_prompt: str, user_prompt: str, 
                        temperature: float = 0.3) -> str:
        """Unified LLM call with fallback chain and error handling."""
        return await self.llm.generate(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=temperature
        )
```

**Change per agent:** Replace `self.client.chat.completions.create(...)` calls with `self._call_llm(...)`.

**Benefit:** All agents automatically get the cost-optimized fallback chain (Gemini free tier â†’ Groq â†’ OpenAI) without per-agent configuration.

---

### A-Q3: Fragile JSON Parsing ðŸŸ  Medium

**Problem:** All agents parse LLM evaluation responses by splitting on triple backticks. No robust extraction.

**Fix â€” Create a Shared JSON Extraction Utility:**

```python
# utils/json_parser.py

import re
import json
from typing import Optional, Dict, Any

def extract_json_from_llm(response_text: str) -> Optional[Dict[str, Any]]:
    """Robustly extract JSON from LLM responses.
    
    Handles: raw JSON, ```json blocks, markdown-wrapped JSON,
    preamble text before JSON, trailing text after JSON.
    """
    # Strategy 1: Try direct parse (cleanest case)
    try:
        return json.loads(response_text.strip())
    except json.JSONDecodeError:
        pass
    
    # Strategy 2: Extract from ```json ... ``` blocks
    pattern = r'```(?:json)?\s*\n?(.*?)\n?\s*```'
    matches = re.findall(pattern, response_text, re.DOTALL)
    for match in matches:
        try:
            return json.loads(match.strip())
        except json.JSONDecodeError:
            continue
    
    # Strategy 3: Find first { ... } or [ ... ] block
    for start_char, end_char in [('{', '}'), ('[', ']')]:
        start_idx = response_text.find(start_char)
        if start_idx == -1:
            continue
        # Find matching closing bracket (handle nesting)
        depth = 0
        for i in range(start_idx, len(response_text)):
            if response_text[i] == start_char:
                depth += 1
            elif response_text[i] == end_char:
                depth -= 1
                if depth == 0:
                    try:
                        return json.loads(response_text[start_idx:i+1])
                    except json.JSONDecodeError:
                        break
    
    return None  # All strategies failed
```

**Usage in agents:**

```python
from utils.json_parser import extract_json_from_llm

# Replace: result = json.loads(response.split("```")[1])
# With:
result = extract_json_from_llm(response_text)
if result is None:
    logger.warning(f"JSON extraction failed for session {session_id}")
    return self._default_evaluation()  # Existing fallback
```

**Change per agent:** ~5 lines per evaluation method (3 agents Ã— ~2 evaluation calls = ~6 changes).

---

### A-Q5: Hardcoded Greetings with Embedded First Question ðŸŸ  Medium

**Problem:** Behavioral and Technical greetings include the first question inline, but `questions_asked` tracking may not correctly record it since the greeting is returned before the interview loop starts.

**Fix â€” Separate Greeting from First Question:**

```python
# In each agent's start_interview() or generate_greeting()

async def start_interview(self, session: InterviewSession) -> dict:
    greeting = self._get_greeting(session)  # Pure welcome, no question
    first_question = session.questions[0]    # First question from bank
    session.questions_asked.append(first_question)  # Track correctly
    
    return {
        "greeting": greeting,
        "first_question": first_question,
        "combined_message": f"{greeting}\n\n{first_question}"
        # Frontend can display as one message or two
    }
```

**Impact:** Frontend may need a minor update to handle the `first_question` field separately, OR use `combined_message` for backward compatibility.

---

### A-Q6: Default Fallback Scores Mask Problems ðŸŸ¡ Low

**Problem:** When LLM evaluation fails, agents silently return 3/5 across all dimensions.

**Fix â€” Add Transparency Flag:**

```python
def _default_evaluation(self, reason: str = "evaluation_unavailable") -> dict:
    return {
        "scores": {dim: 3 for dim in self.evaluation_dimensions},
        "feedback": "We were unable to fully evaluate this response. "
                    "Your score reflects a neutral baseline.",
        "_evaluation_status": "fallback",  # Internal flag
        "_fallback_reason": reason
    }
```

Add logging: `logger.error(f"LLM evaluation failed for session {session_id}: {reason}")`.

The frontend can optionally display a subtle indicator (e.g., "âš ï¸ Partial evaluation") if `_evaluation_status == "fallback"`.

---

### A-Q7: No Input Validation on User Responses ðŸŸ¡ Low

**Problem:** Empty strings, single characters, or gibberish pass directly to evaluation.

**Fix â€” Add Pre-evaluation Validation in `BaseInterviewService`:**

```python
# In BaseInterviewService

def _validate_response(self, response: str) -> tuple[bool, Optional[str]]:
    """Validate user response before sending to LLM evaluation.
    Returns (is_valid, guidance_message_if_invalid).
    """
    cleaned = response.strip()
    
    if not cleaned:
        return False, "It looks like your response was empty. Could you try again?"
    
    if len(cleaned) < 10:
        return False, ("Your response seems quite brief. "
                       "Could you elaborate a bit more?")
    
    return True, None

async def process_response(self, session_id: str, response: str) -> dict:
    is_valid, guidance = self._validate_response(response)
    if not is_valid:
        return {
            "type": "validation_guidance",
            "message": guidance,
            "should_retry": True
        }
    # ... proceed with normal evaluation
```

---

## 3. Module B: Screening Interview Agent Fixes

All fixes in this module apply ONLY to `smartsuccess-interview-backend/interview/screening_interview.py` and its related files.

---

### B-S1: Arbitrary Follow-up Threshold (< 20 Words) ðŸŸ  Medium

**Problem:** Follow-up triggers only when response < 20 words. A 21-word evasive answer gets no follow-up; a 19-word excellent answer gets an unnecessary one.

**Fix â€” Replace Word Count with LLM-Based Completeness Check:**

Since we're already calling the LLM for evaluation (A-Q2 centralizes this), add a `needs_followup` field to the evaluation response schema:

```python
# Add to screening evaluation prompt (already being sent to LLM):

"""
In your JSON response, also include:
"needs_followup": true/false,
"followup_reason": "brief reason if true, null if false"

Set needs_followup to true ONLY if:
- The candidate gave a vague or evasive answer
- Key information was requested but not provided
- The response contradicts earlier statements
Do NOT set it based on response length alone.
"""
```

**In screening_interview.py:**

```python
# Replace:
#   if len(response.split()) < 20: trigger_followup()
# With:
evaluation = await self._evaluate_response(session, question, response)
if evaluation.get("needs_followup", False):
    followup_q = await self._generate_followup(
        session, question, response, evaluation["followup_reason"]
    )
```

**Cost Impact:** $0 â€” the LLM already evaluates the response; adding one boolean field is negligible.

---

### B-S3: Evaluation Criteria Too Generic for Text ðŸŸ  Medium

**Problem:** "Enthusiasm" and "Confidence" are highly subjective for text-based chat. These criteria suit voice/video interviews better.

**Fix â€” Replace with Text-Appropriate Criteria:**

```python
# In screening_interview.py evaluation prompt

# Current criteria:
# Communication Clarity, Relevance, Confidence, Professionalism, Enthusiasm

# Replace with:
SCREENING_CRITERIA = {
    "communication_clarity": "How clearly and coherently did the candidate express their thoughts?",
    "relevance": "How directly did the response address the question asked?",
    "specificity": "Did the candidate provide concrete examples, numbers, or details (vs vague generalities)?",
    "professionalism": "Was the tone and language appropriate for a professional interview?",
    "self_awareness": "Did the candidate demonstrate honest self-reflection and awareness of their strengths/gaps?"
}
```

**Change:** Replace 2 criteria strings in the evaluation prompt. No schema change needed (still 5 dimensions, 1â€“5 scale).

---

### B-S2: No Role/Industry Personalization in Greeting ðŸŸ¡ Low

**Problem:** Greeting is generic regardless of job description provided.

**Fix â€” Template Greeting with Optional JD Context:**

```python
def _get_greeting(self, session: InterviewSession) -> str:
    base = "Welcome! I'm your AI interviewer for this screening session."
    
    if session.job_context:  # If RAG provided JD context
        role = session.job_context.get("job_title", "")
        company = session.job_context.get("company", "")
        if role:
            base += f" We'll be discussing your fit for the {role} position"
            if company:
                base += f" at {company}"
            base += "."
    
    base += (" I'll ask you a few questions to get to know your background "
             "and motivation. Let's get started!")
    return base
```

**Change:** ~10 lines in `screening_interview.py`. No new dependencies.

---

## 4. Module C: Behavioral Interview Agent Fixes

All fixes in this module apply ONLY to `smartsuccess-interview-backend/interview/behavioral_interview.py` and its related files.

---

### C-B1: Follow-up State Leak (Thread Safety) ðŸ”´ High

**Problem:** `follow_up_count` is stored in a service-level `Dict[str, int]`, NOT on the session object. Since the service is a singleton, concurrent sessions share the same dict â€” one user's follow-up count can bleed into another's.

**Fix â€” Move Follow-up State to Session Object:**

```python
# In InterviewSession model (or wherever sessions are defined)

class InterviewSession:
    # ... existing fields ...
    follow_up_count: Dict[int, int] = {}  # key: question_index, value: follow-up count
    # Or simpler:
    follow_ups_used: int = 0  # Total follow-ups used this session
```

**In behavioral_interview.py:**

```python
# Replace:
#   self.follow_up_counts[session_id] = self.follow_up_counts.get(session_id, 0) + 1
# With:
session.follow_ups_used += 1

# Replace:
#   if self.follow_up_counts.get(session_id, 0) >= 2: skip_followup()
# With:
current_q_followups = session.follow_up_count.get(current_index, 0)
if current_q_followups >= 2:
    # Max follow-ups reached for this question, move on
    pass
else:
    session.follow_up_count[current_index] = current_q_followups + 1
```

**Also remove** the service-level dict:

```python
# Delete from BehavioralInterviewService.__init__():
#   self.follow_up_counts: Dict[str, int] = {}
```

**Risk:** Very low â€” only moves data from one dict to another. All other logic unchanged.

---

### C-B2: No STAR Guidance Returned to User ðŸŸ  Medium

**Problem:** The agent detects missing STAR components but only generates a follow-up question. It never explicitly coaches the user ("I noticed you described the situation well but didn't mention your specific actions").

**Fix â€” Add Inline STAR Coaching to Follow-up:**

```python
# In behavioral_interview.py, when generating follow-up

STAR_COACHING_TEMPLATES = {
    "situation": "You've described what you did well, but I'd love more context. "
                 "Could you set the scene â€” what was the specific situation or challenge?",
    "task": "Great context! What specifically was your responsibility or goal in that situation?",
    "action": "I can see the situation clearly. Now, what specific steps did YOU take? "
              "Walk me through your personal actions.",
    "result": "You've explained your approach well. What was the outcome? "
              "Any measurable results or lessons learned?"
}

async def _generate_star_followup(self, session, question, response, 
                                   missing_component: str) -> str:
    coaching = STAR_COACHING_TEMPLATES.get(missing_component, "")
    return coaching  # Direct coaching, no LLM call needed
```

**Cost Impact:** $0 â€” uses template strings, no additional LLM calls.

**Change:** Replace the generic follow-up generation with STAR-specific coaching (~15 lines).

---

### C-B3: Competency Coverage Not Tracked ðŸŸ  Medium

**Problem:** No mechanism ensures all target competencies are covered. A session could accidentally ask 6 teamwork questions.

**Fix â€” Add Competency Tracking to Session:**

```python
# In behavioral_interview.py

TARGET_COMPETENCIES = ["problem_solving", "teamwork", "adaptability", 
                       "leadership", "time_management", "growth_mindset"]

# Map each question in the bank to its primary competency
QUESTION_COMPETENCY_MAP = {
    0: "problem_solving",
    1: "teamwork", 
    2: "adaptability",
    3: "leadership",
    4: "time_management",
    5: "growth_mindset"
}

def _get_next_question(self, session: InterviewSession) -> str:
    """Select next question ensuring competency coverage."""
    covered = set()
    for idx in range(len(session.questions_asked)):
        comp = QUESTION_COMPETENCY_MAP.get(idx)
        if comp:
            covered.add(comp)
    
    # Prioritize uncovered competencies
    uncovered = [c for c in TARGET_COMPETENCIES if c not in covered]
    
    if uncovered and session.current_question_index < len(session.questions):
        # Find a question that covers an uncovered competency
        for q_idx, comp in QUESTION_COMPETENCY_MAP.items():
            if comp in uncovered and q_idx not in session.asked_indices:
                return session.questions[q_idx]
    
    # Fallback: next sequential question
    return session.questions[session.current_question_index]
```

**Change:** ~20 lines in behavioral agent. Adds a mapping dict and selection logic.

---

### C-B4: Greeting Includes First Question but Index Tracking Misaligned ðŸŸ¡ Low

**Problem:** Greeting ends with a question, but `questions_asked[0]` may not match.

**Fix:** This is resolved by **A-Q5** (separate greeting from first question). No additional behavioral-specific fix needed. Just ensure the behavioral agent follows the same pattern as the shared fix.

---

## 5. Module D: Technical Interview Agent Fixes

All fixes in this module apply ONLY to `smartsuccess-interview-backend/interview/technical_interview.py` and its related files.

---

### D-T1: Hardcoded to AI/ML Domain ðŸ”´ High

**Problem:** All 8 domain prompts in `DOMAIN_PROMPTS` are AI/ML-specific. A frontend, DevOps, or data analyst candidate gets irrelevant questions.

**Fix â€” Multi-Domain Question Banks with JD-Based Selection:**

```python
# In rag/technical_rag.py or a new config file

DOMAIN_QUESTION_BANKS = {
    "ai_ml": {
        "domains": ["Python engineering", "LLM frameworks", "RAG architecture",
                     "ML production", "Model training"],
        "questions": [...]  # Existing 8 questions
    },
    "frontend": {
        "domains": ["JavaScript/TypeScript", "React/Vue/Angular", 
                     "CSS/Design Systems", "Performance optimization", "Testing"],
        "questions": [
            "Walk me through how you'd architect a complex single-page application.",
            "How do you approach state management in large frontend applications?",
            # ... 6 more
        ]
    },
    "devops": {
        "domains": ["CI/CD pipelines", "Container orchestration", 
                     "Infrastructure as Code", "Monitoring", "Security"],
        "questions": [...]
    },
    "backend": {
        "domains": ["API design", "Database optimization", "Distributed systems",
                     "Authentication/Authorization", "Caching strategies"],
        "questions": [...]
    },
    "data_engineering": {
        "domains": ["ETL pipelines", "Data warehousing", "Stream processing",
                     "Data quality", "SQL optimization"],
        "questions": [...]
    }
}

def detect_domain_from_jd(job_context: dict) -> str:
    """Detect technical domain from job description keywords."""
    jd_text = (job_context.get("job_description", "") + " " +
               job_context.get("job_title", "")).lower()
    
    domain_keywords = {
        "ai_ml": ["machine learning", "ai ", "llm", "nlp", "deep learning", 
                   "rag", "ml engineer", "data scientist"],
        "frontend": ["frontend", "front-end", "react", "vue", "angular", 
                      "ui developer", "ux engineer"],
        "devops": ["devops", "sre", "infrastructure", "kubernetes", "docker",
                    "ci/cd", "platform engineer"],
        "backend": ["backend", "back-end", "api", "microservices", 
                     "server-side", "java", "go ", "rust"],
        "data_engineering": ["data engineer", "etl", "data pipeline", 
                              "airflow", "spark", "warehouse"]
    }
    
    scores = {}
    for domain, keywords in domain_keywords.items():
        scores[domain] = sum(1 for kw in keywords if kw in jd_text)
    
    best = max(scores, key=scores.get)
    return best if scores[best] > 0 else "ai_ml"  # Default to AI/ML
```

**Change:** Add domain config + detection function (~80 lines), update `technical_interview.py` to use detected domain for question selection (~10 lines).

**Cost Impact:** $0 â€” keyword matching, no LLM calls.

**Risk:** Medium â€” requires creating question banks for new domains. Can start with 2 domains (AI/ML + one more) and expand over time.

**Minimum Viable Fix (if full multi-domain is too much work for Sprint 1):**

```python
# Just add domain detection + a generic fallback
if detected_domain != "ai_ml":
    # Use LLM to generate domain-appropriate questions on the fly
    questions = await self._generate_domain_questions(
        domain=detected_domain, 
        job_context=session.job_context,
        count=8
    )
```

This uses existing Gemini free tier to generate questions dynamically.

---

### D-T2: First Question Assumes Python Expertise ðŸŸ  Medium

**Problem:** "Would you consider yourself an expert-level Python engineer?" â€” inappropriate for non-Python roles.

**Fix â€” Dynamic First Question Based on Detected Domain:**

```python
DOMAIN_OPENERS = {
    "ai_ml": "To start, what programming languages and ML frameworks do you work with most frequently?",
    "frontend": "To start, which frontend frameworks and tools are you most experienced with?",
    "devops": "To start, walk me through your typical infrastructure and deployment stack.",
    "backend": "To start, what backend technologies and architectures are you most comfortable with?",
    "data_engineering": "To start, what data processing tools and platforms do you work with regularly?",
}

# In greeting generation:
domain = detect_domain_from_jd(session.job_context)
opener = DOMAIN_OPENERS.get(domain, DOMAIN_OPENERS["ai_ml"])
```

**Change:** ~10 lines. Depends on D-T1 domain detection.

---

### D-T3: Arbitrary Follow-up Cadence (Even-Indexed Only) ðŸŸ  Medium

**Problem:** Follow-ups only fire on even-numbered questions, regardless of whether the response actually needs probing.

**Fix â€” LLM-Driven Follow-up Decision (Same Pattern as B-S1):**

Add `needs_followup` to the technical evaluation prompt:

```python
# Append to technical evaluation prompt:
"""
Also include in your JSON:
"needs_followup": true/false,
"followup_topic": "specific technical area to probe deeper, if needs_followup is true"

Set needs_followup to true ONLY if:
- The candidate mentioned a technology but didn't explain HOW they used it
- The answer revealed a potential knowledge gap worth exploring
- The candidate made a claim that should be verified with a deeper question
"""
```

Replace the index-based check:

```python
# Replace: if current_index % 2 == 0: generate_followup()
# With:
if evaluation.get("needs_followup", False):
    followup = await self._generate_technical_followup(
        session, question, response, evaluation["followup_topic"]
    )
```

**Cost Impact:** $0 â€” reuses existing evaluation call.

---

### D-T4: `get_question_metadata()` Disconnected from JD ðŸŸ  Medium

**Problem:** The method extracts expected topics from the question text itself, not from the actual job description.

**Fix â€” Inject JD Context into Metadata:**

```python
def get_question_metadata(self, question: str, job_context: dict = None) -> dict:
    """Extract expected topics from question AND job description."""
    # Existing: extract from question text
    base_topics = self._extract_topics_from_question(question)
    
    # New: overlay JD-specific expectations
    if job_context:
        jd_skills = job_context.get("required_skills", [])
        jd_tools = job_context.get("tools", [])
        # Merge JD context into expected topics
        relevant_jd_topics = [s for s in jd_skills + jd_tools 
                              if any(kw in question.lower() for kw in s.lower().split())]
        base_topics["jd_relevant_skills"] = relevant_jd_topics
    
    return base_topics
```

**Change:** ~10 lines in metadata method. Pass `session.job_context` where `get_question_metadata()` is called.

---

### D-T5: No Difficulty Progression ðŸŸ¡ Low

**Problem:** `QuestionDifficulty` enum exists but is never used.

**Fix â€” Add Simple Difficulty Escalation:**

```python
# In technical_interview.py

def _select_next_difficulty(self, session: InterviewSession) -> str:
    """Escalate difficulty based on running average score."""
    if not session.scores:
        return "BASIC"
    
    avg = sum(s.get("technical_accuracy", 3) for s in session.scores) / len(session.scores)
    
    if avg >= 4.0:
        return "ADVANCED"
    elif avg >= 2.5:
        return "INTERMEDIATE"
    else:
        return "BASIC"
```

**Usage:** Tag questions in the bank with difficulty levels, then filter during selection. If no tagged questions are available, fall back to sequential order.

---

## 6. Module E: Customize Interview Agent Fixes (Phase 2)

These fixes apply to `smartsuccess-phase2/gpu-server/` and `smartsuccess-phase2/render-backend/`. They are INDEPENDENT from Phase 1 fixes.

---

### E-A2: Hard GPU Dependency (Single Point of Failure) ðŸŸ  High

**Problem:** Customize Interview is completely unavailable when GPU is offline. No degraded mode.

**Fix â€” Add Cloud Embedding Fallback:**

```python
# In gpu_client.py â€” add fallback for RAG when GPU is offline

class GPUClient:
    async def build_custom_rag(self, files, user_id) -> dict:
        if await self.check_health():
            # GPU available: use CUDA-accelerated pipeline
            return await self._gpu_build_rag(files, user_id)
        else:
            # GPU offline: use cloud fallback
            return await self._cloud_fallback_rag(files, user_id)
    
    async def _cloud_fallback_rag(self, files, user_id) -> dict:
        """Lightweight RAG using Gemini API when GPU is offline.
        
        Instead of GPU embeddings + ChromaDB, uses:
        1. Gemini for text extraction + profile generation (FREE tier)
        2. Simple keyword matching for question customization (no vector search)
        """
        # Extract text from uploaded documents (CPU-only, no GPU needed)
        documents = []
        for file in files:
            text = extract_text_cpu(file)  # PyMuPDF/python-docx work on CPU
            documents.append(text)
        
        combined_text = "\n\n".join(documents)
        
        # Use Gemini to extract profile (replaces GPU-based keyword extraction)
        profile = await self.llm_service.generate(
            system_prompt="Extract a structured profile from these documents.",
            user_prompt=f"Documents:\n{combined_text[:5000]}\n\n"
                        "Return JSON with: skills, career_level, job_target, "
                        "industries, education",
            temperature=0.1
        )
        
        # Generate customized questions using profile (no ChromaDB needed)
        questions = self._customize_questions_from_profile(profile)
        
        return {
            "profile": profile,
            "selected_questions": questions,
            "rag_id": f"fallback_{user_id}",
            "mode": "cloud_fallback"  # Flag for frontend
        }
```

**Cost Impact:** Uses Gemini free tier (1500 req/day). ~500 tokens per profile extraction = negligible.

**Change:** ~60 lines in `gpu_client.py` + minor update in `customize.py` routes.

---

### E-R1: Keyword-Based Profile Extraction ðŸŸ¡ Medium

**Problem:** Profile extraction uses 70+ hardcoded keywords instead of LLM-powered contextual understanding.

**Fix â€” LLM-Powered Profile Extraction (Gemini Free Tier):**

```python
# In rag_service.py (GPU server) or the cloud fallback path

PROFILE_EXTRACTION_PROMPT = """
Analyze the following resume/documents and extract a structured candidate profile.

Documents:
{document_text}

Return a JSON object with:
{{
  "technical_skills": ["skill1", "skill2", ...],
  "soft_skills": ["leadership", "communication", ...],
  "career_level": "junior|mid|senior|principal",
  "years_experience": <number or null>,
  "industries": ["telecom", "fintech", ...],
  "notable_projects": [
    {{"name": "...", "tech_stack": ["..."], "impact": "..."}}
  ],
  "leadership_indicators": ["led team of X", "mentored Y", ...],
  "job_target": "extracted from JD if present",
  "education": [{{"degree": "...", "field": "...", "institution": "..."}}]
}}

Be thorough â€” extract skills from context, not just explicit mentions.
For example, "led a team of 5 engineers" implies leadership AND team management.
"""
```

**Implementation:** Replace the keyword-matching loop in `rag_service.py` with a single Gemini API call. Keep the keyword matcher as a fast fallback if LLM fails.

**Cost:** ~1000 tokens per profile extraction. At Gemini free tier: $0.

---

### E-R2: Template-Based Question Customization ðŸŸ¡ Medium

**Problem:** Questions are customized via string replacement ("interested in your work with {skill1}, {skill2}") instead of genuine personalization.

**Fix â€” LLM-Generated Personalized Questions:**

```python
QUESTION_PERSONALIZATION_PROMPT = """
Given this candidate profile:
{profile_json}

And these base interview questions:
{base_questions_json}

For each question, create a personalized version that:
1. References specific projects, skills, or experiences from their resume
2. Uses their actual job title/target role
3. Adjusts difficulty to their career level ({career_level})

Return JSON array of:
[{{
  "original": "base question text",
  "personalized": "customized question referencing their specific background",
  "category": "screening|behavioral|technical",
  "why": "brief explanation of why this question is relevant to this candidate"
}}]

Example:
- Base: "Walk through the most complex system you've built."
- Personalized: "Your resume mentions building SmartSuccess.AI with FastAPI and ChromaDB. Walk me through the RAG architecture decisions â€” what tradeoffs did you navigate between embedding models?"
"""
```

**Cost:** ~1500 tokens per question set. At Gemini free tier: $0.

**Change:** Replace `_customize_questions()` method (~30 lines). Keep original as fallback.

---

### E-E1: No Customize-Specific Feedback Model ðŸŸ¡ Medium

**Problem:** Uses generic `conversation_engine` for all interview types. Cannot generate a unified scorecard across mixed question categories.

**Fix â€” Add Customize Summary Generator:**

```python
# In render-backend/app/feedback/ (new file: customize_feedback.py)

CUSTOMIZE_SUMMARY_PROMPT = """
This candidate completed a customized interview with mixed question types.

Questions and evaluations:
{qa_evaluations_json}

Candidate profile:
{profile_json}

Generate a comprehensive scorecard:
{{
  "overall_rating": "Strong Hire|Hire|Maybe|No Hire",
  "category_scores": {{
    "screening": {{"avg_score": X, "summary": "..."}},
    "behavioral": {{"avg_score": X, "star_quality": "...", "competencies_demonstrated": [...]}},
    "technical": {{"avg_score": X, "strongest_domain": "...", "gaps": [...]}}
  }},
  "role_fit_assessment": "How well does this candidate match the target role?",
  "top_3_strengths": [...],
  "top_3_growth_areas": [...],
  "hiring_recommendation": "narrative recommendation"
}}
"""
```

**Cost:** ~2000 tokens for summary generation. At Gemini free tier: $0.

---

### E-E2: No RAG-Enhanced Evaluation ðŸŸ¡ Medium

**Problem:** Evaluator doesn't query ChromaDB to cross-reference responses with resume.

**Fix â€” Add Resume Cross-Reference in Evaluation:**

```python
# When evaluating a customize interview response:

async def _evaluate_with_rag_context(self, session, question, response, user_id):
    # Query ChromaDB for relevant resume sections
    rag_context = await self.gpu_client.query_rag(
        user_id=user_id,
        query=question,  # Use the question to find relevant resume sections
        n_results=2
    )
    
    evaluation_prompt = f"""
    ## Resume Context (from candidate's uploaded documents):
    {rag_context}
    
    ## Question Asked:
    {question}
    
    ## Candidate's Response:
    {response}
    
    Evaluate the response. Also note:
    - Does the response align with what their resume states?
    - Did they reference specific experiences mentioned in their documents?
    - Any inconsistencies between resume claims and interview answers?
    
    {standard_evaluation_instructions}
    """
```

**Cost:** 1 additional ChromaDB query per evaluation (local, $0) + ~200 extra tokens in prompt.

---

### E-I1: No Fallback Question Bank When GPU Offline ðŸŸ¡ Medium

**Problem:** If GPU is offline, customize interview fails entirely.

**Fix:** This is resolved by **E-A2** (cloud fallback). The fallback path generates questions using Gemini instead of GPU RAG. Additionally, add a static fallback question bank:

```python
# In render-backend â€” fallback when both GPU and LLM are unavailable

FALLBACK_CUSTOMIZE_QUESTIONS = [
    {"q": "Tell me about yourself and what interests you about this role.", "category": "screening"},
    {"q": "What's your greatest professional achievement?", "category": "screening"},
    {"q": "Describe a challenging project where you had to learn something new quickly.", "category": "behavioral"},
    {"q": "Tell me about a time you disagreed with a team member. How did you resolve it?", "category": "behavioral"},
    {"q": "Walk me through the most complex system you've designed or built.", "category": "technical"},
    {"q": "How do you approach debugging a production issue you've never seen before?", "category": "technical"},
    # ... 4 more to reach 10
]
```

---

### E-R3: No Difficulty Adaptation ðŸŸ¢ Low

Resolved by **E-R2** (LLM-generated questions). The personalization prompt already includes `career_level` for difficulty adjustment.

### E-R4: Limited Question Pool (10 questions) ðŸŸ¢ Low

Resolved by **E-R2** (LLM-generated questions). Gemini can generate novel questions each session instead of selecting from a fixed pool.

### E-I2: Session Store Mismatch Between Phase 1 and Phase 2 ðŸŸ¢ Low

**Fix:** Accept this as architectural debt. Phase 1 and Phase 2 serve different purposes and will eventually merge. No action needed now â€” document the discrepancy in the codebase README.

### E-Phase2-A1: Dual Backend Complexity ðŸŸ¡ Medium

**Fix:** Add a routing configuration document for the frontend:

```python
# In a shared config or the frontend API service

BACKEND_ROUTING = {
    "screening": {"backend": "phase1", "url": PHASE1_API_URL},
    "behavioral": {"backend": "phase1", "url": PHASE1_API_URL},
    "technical": {"backend": "phase1", "url": PHASE1_API_URL},
    "customize": {"backend": "phase2", "url": PHASE2_API_URL},
}
```

Long-term: migrate all interview types to Phase 2 backend. Short-term: document routing clearly.

---

## 7. Module F: Architecture & Infrastructure Fixes

---

### F-A1: In-Memory Session Storage ðŸ”´ High

**Problem:** `self.sessions: Dict[str, InterviewSession] = {}` â€” all sessions lost on restart.

**Fix â€” Add Lightweight File-Based Persistence (Minimum Cost):**

Since the platform runs on Render free tier (512MB RAM), Redis or a database would add cost. Use JSON file persistence as a zero-cost solution:

```python
# services/session_persistence.py

import json
import os
from pathlib import Path

SESSION_DIR = Path("data/sessions")
SESSION_DIR.mkdir(parents=True, exist_ok=True)

class PersistentSessionStore:
    def __init__(self):
        self._cache: Dict[str, InterviewSession] = {}
        self._load_existing()
    
    def _load_existing(self):
        """Load sessions from disk on startup."""
        for f in SESSION_DIR.glob("*.json"):
            try:
                with open(f) as fp:
                    data = json.load(fp)
                self._cache[data["session_id"]] = InterviewSession(**data)
            except Exception:
                pass  # Skip corrupted files
    
    def save(self, session: InterviewSession):
        self._cache[session.session_id] = session
        path = SESSION_DIR / f"{session.session_id}.json"
        with open(path, "w") as fp:
            json.dump(session.to_dict(), fp)
    
    def get(self, session_id: str) -> Optional[InterviewSession]:
        return self._cache.get(session_id)
    
    def delete(self, session_id: str):
        self._cache.pop(session_id, None)
        path = SESSION_DIR / f"{session.session_id}.json"
        path.unlink(missing_ok=True)
    
    def cleanup_old(self, max_age_hours: int = 24):
        """Remove sessions older than max_age_hours."""
        cutoff = datetime.now() - timedelta(hours=max_age_hours)
        for f in SESSION_DIR.glob("*.json"):
            if datetime.fromtimestamp(f.stat().st_mtime) < cutoff:
                f.unlink()
                # Also remove from cache
                sid = f.stem
                self._cache.pop(sid, None)
```

**In BaseInterviewService:**

```python
class BaseInterviewService:
    def __init__(self):
        self.sessions = PersistentSessionStore()
        # Replace: self.sessions: Dict[str, InterviewSession] = {}
```

**Cost:** $0 â€” uses filesystem (Render free tier includes persistent disk with a paid plan, but even without it, sessions survive within a single deployment cycle).

**Note:** For true persistence across Render free tier restarts, consider Render's free PostgreSQL (90 days) or a free Supabase/Neon database. But file-based is the minimum-effort solution.

---

### F-A2: Singleton Pattern Not Thread-Safe ðŸŸ  Medium

**Problem:** Global `_screening_service_instance` with `Optional` check has race conditions.

**Fix â€” Use Python's Built-in Thread-Safe Singleton:**

```python
# Replace the manual singleton pattern in each service:

# Before (unsafe):
_instance = None
def get_screening_service():
    global _instance
    if _instance is None:
        _instance = ScreeningInterviewService()
    return _instance

# After (thread-safe, using functools):
from functools import lru_cache

@lru_cache(maxsize=1)
def get_screening_service() -> ScreeningInterviewService:
    return ScreeningInterviewService()
```

`lru_cache` is thread-safe in CPython due to the GIL. For async contexts, this is sufficient.

**Change:** 3 lines per agent (3 agents = 9 lines total).

---

### F-A3: No Rate Limiting on LLM Calls ðŸŸ  Medium

**Problem:** Each user message triggers 1â€“2 LLM calls with no throttling.

**Fix â€” Add Simple In-Memory Rate Limiter:**

```python
# utils/rate_limiter.py

from collections import defaultdict
from time import time

class SimpleRateLimiter:
    def __init__(self, max_calls_per_minute: int = 30):
        self.max_calls = max_calls_per_minute
        self.calls: Dict[str, list] = defaultdict(list)
    
    def check(self, user_id: str) -> bool:
        """Returns True if the call is allowed."""
        now = time()
        # Clean old entries
        self.calls[user_id] = [t for t in self.calls[user_id] if now - t < 60]
        
        if len(self.calls[user_id]) >= self.max_calls:
            return False
        
        self.calls[user_id].append(now)
        return True

# Usage in BaseInterviewService:
rate_limiter = SimpleRateLimiter(max_calls_per_minute=30)

async def process_response(self, session_id, user_id, response):
    if not rate_limiter.check(user_id):
        return {"error": "Too many requests. Please wait a moment."}
    # ... proceed
```

**Cost:** $0. ~20 lines of code.

---

### F-A4: SessionStore Dual-Tracking ðŸŸ¡ Low

**Problem:** Sessions stored in both `self.sessions` (base class) and `SessionStore` (Phase 2).

**Fix:** With F-A1 (PersistentSessionStore), this becomes the single source of truth. Remove any Phase 2 `SessionStore` sync calls from Phase 1 code if present. Document in code comments that Phase 1 and Phase 2 have separate session stores by design.

---

## 8. Module G: Prompt Engineering Fixes

---

### G-P1: No System Prompt for Evaluator ðŸŸ  Medium

**Problem:** Evaluation prompts sent as user messages, not system role.

**Fix â€” Restructure LLM Calls to Use System Prompt:**

With A-Q2 (centralized `_call_llm()`), this is straightforward:

```python
# All evaluation calls now use system_prompt parameter:

evaluation_system_prompt = """
You are an expert interview evaluator. You assess candidate responses 
objectively and consistently. Always return valid JSON matching the 
requested schema. Never add preamble or explanation outside the JSON.
"""

result = await self._call_llm(
    system_prompt=evaluation_system_prompt,  # Now properly set
    user_prompt=evaluation_prompt_with_question_and_response,
    temperature=0.3
)
```

**Change:** Add 1 system prompt constant + update `_call_llm()` calls in each agent.

---

### G-P2: JSON-Only Output Instruction is Fragile ðŸŸ  Medium

**Problem:** "Return ONLY the JSON" doesn't reliably prevent LLM preamble.

**Fix â€” Already Partially Resolved by A-Q3 (robust JSON parser).**

Additionally, strengthen the prompt instruction:

```python
# Add to evaluation prompts:
"""
CRITICAL: Your entire response must be a single valid JSON object.
Do not include any text before or after the JSON.
Do not wrap in markdown code blocks.
Start your response with { and end with }.
"""
```

For Gemini specifically, use `response_mime_type="application/json"` if the SDK supports it (Gemini 1.5+ does).

---

### G-P3: No Few-Shot Examples in Evaluation Prompts ðŸŸ  Medium

**Problem:** No examples of 1/5 vs 5/5 responses, leading to score inflation.

**Fix â€” Add Calibration Examples to Evaluation Prompts:**

```python
SCORING_CALIBRATION = """
## Scoring Calibration Examples:

**Score 1/5 (Poor):** "I guess I like teamwork." 
â†’ No specifics, no evidence, extremely brief.

**Score 3/5 (Adequate):** "In my last role, I worked with a team on a project. 
We had some disagreements but we figured it out and delivered on time."
â†’ Has basic structure but lacks specific details, actions, and measurable outcomes.

**Score 5/5 (Excellent):** "At Bell Mobility, I led a cross-functional team of 8 
engineers to migrate our RF propagation modeling system to AWS. When we hit a 
conflict between the infrastructure and ML teams on container sizing, I organized 
a data-driven comparison that showed 40% cost savings with the ML team's approach. 
We delivered 2 weeks early and reduced monthly compute costs by $12K."
â†’ Specific situation, clear actions, quantified results, demonstrates multiple competencies.
"""
```

Append this to each evaluation prompt. ~150 extra tokens per call.

---

### G-P4: Temperature Too Low for Question Generation ðŸŸ¡ Low

**Problem:** Both question generation and evaluation use `temperature=0.3`.

**Fix â€” Use Different Temperatures:**

```python
# In _call_llm() or where temperature is set:

# For evaluation (consistency matters): temperature=0.2
# For question generation (diversity matters): temperature=0.7
# For follow-up generation (moderate creativity): temperature=0.5

async def _evaluate_response(self, ...):
    return await self._call_llm(..., temperature=0.2)

async def _generate_question(self, ...):
    return await self._call_llm(..., temperature=0.7)
```

**Change:** 1 parameter per call. ~6 changes total.

---

## 9. Module H: User Experience Fixes

---

### H-U1: No Real-Time Feedback During Interview ðŸŸ  Medium

**Problem:** Users only see scores at the end.

**Fix â€” Add Lightweight Inline Hints (No Extra LLM Call):**

Since evaluation already happens per-response, extract a one-line coaching hint from the existing evaluation:

```python
# Add to evaluation prompt:
"""
Also include:
"coaching_hint": "One brief, encouraging sentence of advice for the candidate's 
next response. Focus on what they could ADD, not what they did wrong.
Example: 'Try including a specific metric or outcome in your next answer.'"
"""

# In the response to the frontend:
return {
    "evaluation": scores,
    "coaching_hint": evaluation.get("coaching_hint", None),
    "next_question": next_q
}
```

Frontend displays the hint subtly (e.g., a small coaching bubble) before the next question.

**Cost:** $0 â€” reuses existing LLM call (adds ~20 tokens to response).

---

### H-U2: End-of-Interview Detection is Brittle ðŸŸ¡ Low

**Problem:** Substring match for "stop", "end", "done" triggers false positives.

**Fix â€” Use Exact Phrases with Confirmation:**

```python
# Replace substring matching with exact-phrase + confirmation flow

END_PHRASES = {"end interview", "stop interview", "i want to stop", "quit interview"}

def _check_end_request(self, response: str) -> bool:
    cleaned = response.strip().lower()
    # Must be the entire message or start with the phrase
    return any(cleaned == phrase or cleaned.startswith(phrase + ".") 
               for phrase in END_PHRASES)

# When detected, ask for confirmation:
if self._check_end_request(response):
    return {
        "type": "end_confirmation",
        "message": "Would you like to end the interview? "
                   "You've completed {n} of {total} questions. "
                   "Type 'yes' to confirm or continue with your answer."
    }
```

---

### H-U3: No Pause/Resume Capability ðŸŸ¡ Low

**Problem:** No way to pause and resume an interview.

**Fix â€” Add Simple Pause State to Session:**

```python
# In InterviewSession:
class InterviewSession:
    # ... existing ...
    is_paused: bool = False
    paused_at: Optional[datetime] = None

# In process_response():
PAUSE_PHRASES = {"pause", "pause interview", "brb", "be right back"}

if response.strip().lower() in PAUSE_PHRASES:
    session.is_paused = True
    session.paused_at = datetime.now()
    return {
        "type": "paused",
        "message": "Interview paused. Type anything when you're ready to continue."
    }

# On next message, if paused:
if session.is_paused:
    session.is_paused = False
    elapsed = datetime.now() - session.paused_at
    return {
        "type": "resumed",
        "message": f"Welcome back! You were away for {elapsed.seconds // 60} minutes. "
                   f"Let's continue with the next question.",
        "next_question": current_question
    }
```

---

### H-U4: Completion Message is Generic ðŸŸ¡ Low

**Problem:** All candidates get the same "Great job!" regardless of performance.

**Fix â€” Performance-Tiered Completion Messages:**

```python
def _get_completion_message(self, avg_score: float, interview_type: str) -> str:
    if avg_score >= 4.0:
        return ("Excellent work! You demonstrated strong skills throughout this "
                f"{interview_type} interview. Your responses showed depth and clarity.")
    elif avg_score >= 3.0:
        return ("Good effort! You showed solid foundations in this "
                f"{interview_type} interview. Review the feedback below "
                "for areas where you can strengthen your responses.")
    elif avg_score >= 2.0:
        return ("Thank you for completing this {interview_type} interview. "
                "The feedback below highlights specific areas for improvement. "
                "Practice with the suggested focus areas and try again!")
    else:
        return ("Thank you for your effort in this {interview_type} interview. "
                "Don't be discouraged â€” review the detailed feedback below "
                "and consider practicing each area individually.")
```

---

## 10. Module I: Data & Evaluation Quality Fixes

---

### I-D1: No Evaluation Calibration Across Models ðŸŸ  Medium

**Problem:** Different LLM models (GPT-4o-mini vs Gemini) may score the same response differently.

**Fix â€” Add Score Normalization with Model Offset:**

```python
# utils/score_calibration.py

# Empirically determined offsets (calibrate by running same responses through each model)
MODEL_SCORE_OFFSETS = {
    "gemini-2.0-flash": 0.0,      # Baseline
    "gemini-1.5-flash": -0.1,     # Slightly harsher
    "gpt-4o-mini": +0.2,          # Slightly more generous
    "groq-llama": -0.3,           # Notably harsher
}

def normalize_score(raw_score: float, model_name: str) -> float:
    """Normalize score to account for model-specific bias."""
    offset = MODEL_SCORE_OFFSETS.get(model_name, 0.0)
    normalized = raw_score - offset
    return max(1.0, min(5.0, round(normalized, 1)))  # Clamp to 1-5
```

**How to calibrate:** Run 10 representative responses through each model, average the scores, and compute offsets relative to the baseline model.

**Cost:** $0 (one-time calibration exercise).

---

### I-D2: Score Averaging Loses Nuance ðŸŸ¡ Low

**Problem:** Simple averaging groups inconsistent candidates with consistent mediocre ones.

**Fix â€” Add Score Distribution Metrics:**

```python
import statistics

def compute_score_summary(scores: List[float]) -> dict:
    avg = statistics.mean(scores)
    return {
        "average": round(avg, 2),
        "min_score": min(scores),
        "max_score": max(scores),
        "std_dev": round(statistics.stdev(scores), 2) if len(scores) > 1 else 0,
        "consistency": "high" if statistics.stdev(scores) < 0.5 else 
                       "moderate" if statistics.stdev(scores) < 1.0 else "low",
        "trend": "improving" if scores[-1] > scores[0] else 
                 "declining" if scores[-1] < scores[0] else "stable"
    }
```

**Change:** ~15 lines. Add to summary generation in each agent.

---

### I-D3: No Anti-Gaming Measures ðŸŸ¡ Low

**Problem:** Users could paste AI-generated answers.

**Fix â€” Add Basic Detection Heuristics (No ML Required):**

```python
# utils/response_analytics.py

def detect_potential_gaming(response: str, response_time_seconds: float) -> dict:
    """Basic heuristics to flag potentially gamed responses."""
    flags = []
    
    # Flag 1: Extremely fast, long response (likely paste)
    words = len(response.split())
    if words > 100 and response_time_seconds < 10:
        flags.append("rapid_long_response")
    
    # Flag 2: Overly formal/structured for a chat (AI-generated indicator)
    formal_markers = ["in conclusion", "furthermore", "it is worth noting",
                      "in summary", "firstly", "secondly", "thirdly"]
    formal_count = sum(1 for m in formal_markers if m in response.lower())
    if formal_count >= 3:
        flags.append("overly_structured")
    
    # Flag 3: Perfect formatting (bullet points in chat)
    if response.count("\n- ") >= 3 or response.count("\nâ€¢ ") >= 3:
        flags.append("formatted_response")
    
    return {
        "flags": flags,
        "is_suspicious": len(flags) >= 2,
        "confidence": "low"  # These are heuristics, not definitive
    }
```

**Usage:** Log flags for analytics. Do NOT penalize users based on heuristics alone â€” just track patterns.

---

## 11. Implementation Roadmap

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IMPLEMENTATION ROADMAP                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          â”‚                                                        â”‚
â”‚ Sprint 1 â”‚  ðŸ”´ CRITICAL & HIGH FIXES                             â”‚
â”‚ (Week 1) â”‚  â”œâ”€ A-Q4: Conversation context in evaluation          â”‚
â”‚          â”‚  â”œâ”€ C-B1: Follow-up state â†’ session object             â”‚
â”‚          â”‚  â”œâ”€ D-T1: Multi-domain question banks (MVP)            â”‚
â”‚          â”‚  â”œâ”€ F-A1: File-based session persistence               â”‚
â”‚          â”‚  â””â”€ E-A2: GPU offline fallback for Customize           â”‚
â”‚          â”‚                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          â”‚                                                        â”‚
â”‚ Sprint 2 â”‚  ðŸŸ  SHARED FOUNDATION (All agents benefit)             â”‚
â”‚ (Week 2) â”‚  â”œâ”€ A-Q2: Centralize LLM calls â†’ llm_service.py      â”‚
â”‚          â”‚  â”œâ”€ A-Q3: Robust JSON parser utility                   â”‚
â”‚          â”‚  â”œâ”€ A-Q1: Delete dead prompt files                     â”‚
â”‚          â”‚  â”œâ”€ A-Q5: Separate greeting from first question        â”‚
â”‚          â”‚  â”œâ”€ G-P1: System prompt for evaluator                  â”‚
â”‚          â”‚  â”œâ”€ G-P2: Strengthen JSON output instructions          â”‚
â”‚          â”‚  â””â”€ G-P3: Few-shot calibration examples                â”‚
â”‚          â”‚                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          â”‚                                                        â”‚
â”‚ Sprint 3 â”‚  ðŸŸ  AGENT-SPECIFIC MEDIUM FIXES                       â”‚
â”‚ (Week 3) â”‚  â”œâ”€ B-S1: LLM-based follow-up decision (Screening)    â”‚
â”‚          â”‚  â”œâ”€ B-S3: Text-appropriate criteria (Screening)        â”‚
â”‚          â”‚  â”œâ”€ C-B2: STAR coaching templates (Behavioral)         â”‚
â”‚          â”‚  â”œâ”€ C-B3: Competency coverage tracking (Behavioral)    â”‚
â”‚          â”‚  â”œâ”€ D-T2: Dynamic first question (Technical)           â”‚
â”‚          â”‚  â”œâ”€ D-T3: LLM-driven follow-up (Technical)            â”‚
â”‚          â”‚  â””â”€ D-T4: JD-aware question metadata (Technical)       â”‚
â”‚          â”‚                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          â”‚                                                        â”‚
â”‚ Sprint 4 â”‚  ðŸŸ¡ CUSTOMIZE INTERVIEW ENHANCEMENTS (Phase 2)         â”‚
â”‚ (Week 4) â”‚  â”œâ”€ E-R1: LLM-powered profile extraction              â”‚
â”‚          â”‚  â”œâ”€ E-R2: LLM-generated personalized questions         â”‚
â”‚          â”‚  â”œâ”€ E-E1: Customize-specific feedback model            â”‚
â”‚          â”‚  â”œâ”€ E-E2: RAG-enhanced evaluation                      â”‚
â”‚          â”‚  â”œâ”€ E-I1: Fallback question bank                       â”‚
â”‚          â”‚  â””â”€ E-Phase2-A1: Backend routing documentation         â”‚
â”‚          â”‚                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          â”‚                                                        â”‚
â”‚ Sprint 5 â”‚  ðŸŸ¡ðŸŸ¢ UX + LOW PRIORITY + POLISH                      â”‚
â”‚ (Week 5) â”‚  â”œâ”€ H-U1: Inline coaching hints                       â”‚
â”‚          â”‚  â”œâ”€ H-U2: End-detection fix                            â”‚
â”‚          â”‚  â”œâ”€ H-U3: Pause/resume                                 â”‚
â”‚          â”‚  â”œâ”€ H-U4: Performance-tiered completion                â”‚
â”‚          â”‚  â”œâ”€ A-Q6: Fallback score transparency                  â”‚
â”‚          â”‚  â”œâ”€ A-Q7: Input validation                             â”‚
â”‚          â”‚  â”œâ”€ B-S2: Personalized greeting (Screening)            â”‚
â”‚          â”‚  â”œâ”€ D-T5: Difficulty progression (Technical)           â”‚
â”‚          â”‚  â”œâ”€ F-A2: Thread-safe singletons                       â”‚
â”‚          â”‚  â”œâ”€ F-A3: Rate limiting                                â”‚
â”‚          â”‚  â”œâ”€ G-P4: Temperature tuning                           â”‚
â”‚          â”‚  â”œâ”€ I-D1: Score normalization                          â”‚
â”‚          â”‚  â”œâ”€ I-D2: Score distribution metrics                   â”‚
â”‚          â”‚  â””â”€ I-D3: Anti-gaming heuristics                       â”‚
â”‚          â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 12. Cost Impact Summary

| Fix | Additional LLM Tokens | Additional Cost |
|-----|:---:|:---:|
| A-Q4 (conversation context) | +200â€“400/eval | $0 (Gemini free) |
| B-S1 (LLM follow-up decision) | +20/eval (1 field) | $0 |
| D-T1 (dynamic question gen) | +500/session (if LLM) | $0 |
| D-T3 (LLM follow-up decision) | +20/eval (1 field) | $0 |
| E-A2 (cloud fallback RAG) | +500/session | $0 |
| E-R1 (LLM profile extraction) | +1000/upload | $0 |
| E-R2 (LLM question generation) | +1500/session | $0 |
| E-E1 (customize feedback) | +2000/session | $0 |
| G-P3 (few-shot examples) | +150/eval | $0 |
| H-U1 (coaching hints) | +20/eval (1 field) | $0 |
| All other fixes | 0 (code-only) | $0 |
| **Total per session** | **~2,000â€“5,000 extra tokens** | **$0** |

**All fixes operate within the Gemini free tier (1,500 requests/day).** Even at maximum usage (50 complete interview sessions/day), the total token consumption stays well under free tier limits.

**Infrastructure cost remains: $0â€“10/month** (unchanged from current architecture).

---

## Quick Reference: Fix Count by Module

| Module | Fixes | Critical/High | Medium | Low |
|--------|:---:|:---:|:---:|:---:|
| A: Cross-Agent Shared | 7 | 1 | 4 | 2 |
| B: Screening Agent | 3 | 0 | 2 | 1 |
| C: Behavioral Agent | 4 | 1 | 2 | 1 |
| D: Technical Agent | 5 | 1 | 3 | 1 |
| E: Customize Agent (Phase 2) | 10 | 1 | 6 | 3 |
| F: Architecture & Infra | 4 | 1 | 2 | 1 |
| G: Prompt Engineering | 4 | 0 | 3 | 1 |
| H: User Experience | 4 | 0 | 1 | 3 |
| I: Data & Evaluation | 3 | 0 | 1 | 2 |
| **Total** | **44** | **5** | **24** | **15** |

---

*Plan generated: February 15, 2026*  
*Repository: https://github.com/EmmaW215/SmartSccuss_Career_Intelligence_AI*  
*Based on: SmartSuccess_Mock_Interview_Agents_Analysis.md + SmartSuccess_Customize_Interview_Reassessment.md*
